{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "This homework contains two parts, Part 1 on Linear Regression and Part 2 on Logistic Regression. Both parts use real world data and will introduce you to techniques used in the workforce! As a reminder, DO NOT edit anything in this python notebook! All of your code will be contained in the functions located in answers.py. You should be able to complete this assignment with no extra imports so please use what we have given you.\n",
    "\n",
    "Some functions you may want to take note of before you start:\n",
    "- Pandas .corr() function to calculate correlation\n",
    "- Pandas .mean(), .median(), .std() functions\n",
    "- Pandas mapping a lambda function eg .map(lambda x: x)\n",
    "- ' '.join(x) function where x is a list\n",
    "- sklearn train_test_split() function\n",
    "- Python strings .isalpha() function\n",
    "- sklearns confusion_matrix() function\n",
    "- Pandas .plot() function\n",
    "- statsmodels .summary() function\n",
    "- numpy .linspace() function\n",
    "- scipy norm.fit(), norm.pdf() functions\n",
    "- statsmodels qqplot() function\n",
    "- statsmodels .predict() function\n",
    "- random.normal() function\n",
    "\n",
    "Make sure you have all of the packages installed, if you do not use this command: \"pip install pandas numpy sklearn matplotlib statsmodels nltk scipy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all of the necessary packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.stats import norm\n",
    "import nltk\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from answers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Part 1\n",
    "Part 1 of the homework will focus applying Linear Regression to real world data. \n",
    "<br>\n",
    "McDonald's is a global fast food chain that serves hamburgers in 119 different countries. McDonald's flagship hamburger is the Big Mac. Just like all other things in this world, the cost of the Big Mac fluctuates with the enconomy, however, the Big Mac is in a unique position by its presence in over half of the worlds countries. Through this exercise we will use linear regression to see if global markets data can be used to predict the cost of a Big Mac.\n",
    "\n",
    "This is part one of the coding assignment, it will go through cleaning the data we have provided to you and then training a linear regression model on this data to predict the dollar_price variable. The columns in this dataset we will be concerned with are local_price (the price of the big mac in native currency), dollar_ex (the exchange rate of the dollar from that native currency), the dollar_price (the converted dollar value of a big mac), GDP_dollar (the GDP of the dollar at sample time), adj_price (the adjusted big mac price), and USD, EUR, GBP, JPY, and CNY (the values of various world currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the county data as a pandas DataFrame\n",
    "bmdf = pd.read_csv(\"big-mac-adjusted-index.csv\")\n",
    "bmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the columns that are present, which ones we will be using as independent, and that the dependent column is dollar_price\n",
    "indepCols = ['local_price', 'dollar_ex', 'GDP_dollar', 'adj_price', 'USD', 'EUR', 'GBP', 'JPY', 'CNY']\n",
    "depCol = 'dollar_price'\n",
    "bmdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at some of the attributes of our DataFrame and become familiar with the data\n",
    "bmdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 (2 pts.) \n",
    "# Edit the python function in answers.py to find all of the rows where dollar_price is zero and return that DataFrame\n",
    "# RETURN: Modified DataFrame\n",
    "findZeroDollarPrice(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 (1 pts.)\n",
    "# It is invalid for a Big Mac to be free (dollar_price = zero) so we will replace the dollar price with NaN (similar to null or \n",
    "# None) use the replace method to replace 0 in the dollar_price column with np.NaN. We do this so the invalid values will not \n",
    "# mess with our mean, median, or standard deviation metrics when replacing these invalid values.\n",
    "# RETURN: Modified DataFrame\n",
    "bmdf = replaceZeroWithNaN(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make a test copy for our DataFrame and test a few ways we could try to fix the dollar price NaN values.\n",
    "# Note that pandas is smart, NaN will not be factored into any sort of mean, median, std calculations so we don't have to worry\n",
    "# about this messing with our experiments.\n",
    "bmdfTest = bmdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a function to print the Mean, Standard Deviation, and Median. Note the shortcuts you can use to calculate these\n",
    "# values for a column. We will use this function to show how different handelings of NaN values can influence our dataset, for\n",
    "# example printed from this cell is the metrics for dollar_price if NaN was not counted at all.\n",
    "def printMeanStdMedian(bmdfTest):\n",
    "    print(f'mean: {bmdfTest[\"dollar_price\"].mean()}, std: {bmdfTest[\"dollar_price\"].std()}, median:{bmdfTest[\"dollar_price\"].median()}')\n",
    "printMeanStdMedian(bmdfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 (1 pts.) \n",
    "# Fill in the function replaceNaNWithZero that will replace the dollar_price NaN values with zero\n",
    "# RETURN: Modified DataFrame\n",
    "bmdfTest = replaceNaNWithZero(bmdfTest)\n",
    "printMeanStdMedian(bmdf)\n",
    "bmdfTest = bmdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 (1 pts.) \n",
    "# Fill in the function replaceNaNWithMean that will replace the dollar_price NaN values with the dollar_price mean\n",
    "# RETURN: Modified DataFrame\n",
    "bmdfTest = replaceNaNWithMean(bmdfTest)\n",
    "printMeanStdMedian(bmdfTest)\n",
    "bmdfTest = bmdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 (1 pts.) \n",
    "# Fill in the function replaceNaNWithMedian that will replace the dollar_price NaN values with the dollar_price median \n",
    "# RETURN: Modified DataFrame\n",
    "bmdfTest = replaceNaNWithMedian(bmdfTest)\n",
    "printMeanStdMedian(bmdfTest)\n",
    "bmdfTest = bmdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 (2 pts.) \n",
    "# Fill in the function replaceNaNWithNormal that will replace the dollar_price NaN values with random samples from a normal\n",
    "# distribution with the same mean and standard deviation as dollar_price\n",
    "# (hint: use random.normal() to generate values to replace the NaN values)\n",
    "# RETURN: Modified DataFrame\n",
    "bmdfTest = replaceNaNWithNormal(bmdfTest)\n",
    "printMeanStdMedian(bmdfTest)\n",
    "bmdfTest = bmdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we have decided on filling our invalid values with the mean\n",
    "bmdf = replaceNaNWithMean(bmdf)\n",
    "printMeanStdMedian(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1.7 (3 pts.)\n",
    "# To get a better idea of our data, we now want to graph scatter plots of each independent variable vs our \n",
    "# dependent variable dollar_price. Fill out the function graphIndepVsDep by graphing each independent variable against\n",
    "# dollar_price. Set a title for each graph and label the x and y axis. \n",
    "# Make sure to display the plots below this cell\n",
    "# RETURN: Nothing, display graphs\n",
    "graphIndepVsDep(bmdf, indepCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 (1 pts.) \n",
    "# Interesting... there seems to be two points in particular with a significantly higher (>60,000) local_price. Fill in the\n",
    "# localPriceOutlier function to return the two points rows as a dataframe.\n",
    "# RETURN: DataFrame with outliers\n",
    "localPriceOutlier(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9 (1 pts.)\n",
    "# There seems to be a similar occurance with the dollar_ex variable, fill out the dollarExOutliers function to output all\n",
    "# the information about rows with more than 20,000 dollar_ex\n",
    "# RETURN: DataFrame with outliers\n",
    "dollarExOutliers(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.10 (3 pts.) \n",
    "# Lets check the correlation between all of our numeric variables. Fill in the function correlationHeatmap that will calculate\n",
    "# the correlation matrix of all our variables and then graph them as a heatmap. \n",
    "# Make sure to display the plot below this cell\n",
    "# RETURN: Nothing, display a graph\n",
    "correlationHeatmap(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.11 (3 pts.) \n",
    "# Now lets do some Linear Regression! Fill in the linearRegressionFit function using the ols and fit function from statsmodels.\n",
    "# Use GDP_dollar and USD as independent variables and dollar_price as the dependent variable.\n",
    "# Print the model summary as part of the linearRegressionFit function. linearRegressionFit should return the fitted model.\n",
    "# RETURN: Linear Regression model\n",
    "model = linearRegressionFit(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the actual weights assigned to our parameters\n",
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the p values assigned to our parameters, they are low because both variables have a significant effect on the model\n",
    "model.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can even look at the rsquared value\n",
    "model.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will add the residuals to our bmdf DataFrame for further analysis\n",
    "bmdf['residual']=model.resid\n",
    "bmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.12 (3 pts.) \n",
    "# We would like to see how our independent variables relate to the residuals, fill out the function graphIndepVsResidual\n",
    "# to generate a scatter plot for each independent variable passed in where the residuals are on the y axis and the independent\n",
    "# variable is on the x axis. Make sure to include a title as well as labels for the x and y axis. \n",
    "# Make sure to display the plots below this cell\n",
    "# RETURN: Nothing, display a graph\n",
    "graphIndepVsResidual(bmdf, ['GDP_dollar', 'USD'], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.13 (5 pts.) \n",
    "# Fill in the function histOfResiduals to generate a histogram of the residuals and overlay a normal curve on top of this\n",
    "# histogram. Us norm.fit, np.linspace and norm.pdf to generate the data for a normal curve and make sure to set density=True \n",
    "# when plotting the histogram. Be sure to title your plot as well as label the x and y axis\n",
    "# Make sure to display the plot below this cell\n",
    "# RETURN: Nothing, display a graph\n",
    "histOfResiduals(bmdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.14 (3 pts.) \n",
    "# Using the qqplot function imported from statsmodels graph the QQ Plot of the model residuals with a 45 degree line (you can\n",
    "# add such a line in the arguments of the qqplot function, make sure to set fit=True)\n",
    "# Make sure to display the plot below this cell\n",
    "# RETURN: Nothing, display a graph\n",
    "graphQQPlot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.15 (3 pts.) \n",
    "# Using the model, access the fitted values and residual values to fill in the graphFittedVsResidual which graphs a scatter\n",
    "# plot of fitted values on the x axis vs residual values on the y axis. Make sure to add a title and label the x and y axis\n",
    "# Make sure to display the plot below this cell\n",
    "# RETURN: Nothing, display a graph\n",
    "graphFittedVsResidual(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.16 (2 pts.) \n",
    "# Using the model.predict function to fill out the predictLinearRegression function that takes in the model, prints out the\n",
    "# data that you are predicting and the predicted value. \n",
    "# RETURN: Floating point predicted number\n",
    "predictLinearRegression(model, 1000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Part 2\n",
    "In this section we will be predicting whether a movie review from IMDB is positive or negative using logistic regression! The IMDB movie review dataset is a popular sentiment analysis dataset that is used in many parts of machine learning. To keep this assignment manageable, we have provided a subset of 6,000 movie reviews as well as their associated labels positive (1) and negative (0) in imdbReviews.csv. These labels were made and validated by humans so we accept them as a ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the corpus\n",
    "df = pd.read_csv('imdbReviews.csv')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will give you a random row from our DataFrame, feel free to run it as many times as you like to get a feel for \n",
    "# what is generally labeled as negative and what is considered positive.\n",
    "sample = df.sample()\n",
    "print(f'Score: {sample[\"label\"].values[0]}\\n\\nText: {sample[\"text\"].values[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 (3 pts.)\n",
    "# Now we need to edit the text so that it will be simpler for us to vectorize it and run logistic regression. The first\n",
    "# modification we will make is sending all of the text to lower case. Fill out the lowerCase function and return the\n",
    "# DataFrame with all of the text in lower case. (Hint: the map function in pandas may be helpful)\n",
    "# RETURN : the modified DataFrame, make all edits within the text column\n",
    "df = lowerCase(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 (3 pts.) \n",
    "# Next, we are going to tokenize our text so we can filter out unwanted pieces of the sentence. Fill out the tokenizeDF function\n",
    "# so that all of the text is tokenized using word_tokenize(). (Hint: the map function in pandas may be helpful)\n",
    "# RETURN : the modified DataFrame, make all edits within the text column\n",
    "df = tokenizeDF(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 (4 pts.) \n",
    "# Now, we are going to filter through the tokenized text and in each row removing all of the stop words.\n",
    "# Fill out the removeStop function to do this and return the modified DataFrame. We also pass in a set of stop words\n",
    "# in english.\n",
    "# RETURN : the modified DataFrame, make all edits within the text column\n",
    "stopeng = set(stopwords.words('english'))\n",
    "df = removeStop(df, stopeng)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 (4 pts.) \n",
    "# Now, we are going to filter through the tokenized text and in each row only keeping words that have any\n",
    "# alphabetic characters in them, and removing any tokens that are of size 0 or 1.\n",
    "# Fill out the removeStopKeepAlpha function to do this and return the modified DataFrame. We also pass in a set of stop words\n",
    "# in english.\n",
    "# RETURN : the modified DataFrame, make all edits within the text column\n",
    "df = keepAlpha(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 (3 pts.) \n",
    "# Lastly, we will join all of the tokenized words per row back into single strings. Fill out the joinText function to join\n",
    "# your tokenized words together and return the new DataFrame\n",
    "# RETURN : the modified DataFrame, make all edits within the text column \n",
    "df = joinText(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 (4 pts.)\n",
    "# We need to vectorize our new text so that we can use it as input for the logistic regression. Fill in the function\n",
    "# countVectorize to turn our DataFrame df into a matrix X which has the shape (num of examples, vocabulary size) and array\n",
    "# y which has the shape (num of examples). Use the CountVectorizer method to achieve this.\n",
    "# RETURN: X, y, and vectorizer\n",
    "X, y, vectorizer = countVectorize(df)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 (3 pts.) \n",
    "# Fill in the function splitData using the train_test_split function to get a test size of .25 and set the random state to 42.\n",
    "X_train, X_test, y_train, y_test = splitData(X, y)\n",
    "print(f'XTrain length: {len(X_train)}, XTest length: {len(X_test)}, YTrain length: {len(y_train)}, YTest length: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make sure our variables are all of the correct type.\n",
    "y_test = y_test.astype(int)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "X_test = X_test.astype(int)\n",
    "X_train = X_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.8 (4 pts.) \n",
    "# Fill in the function trainLogisticRegression by creating a LogisticRegression object with a random state of 42 and then\n",
    "# fitting that object to X_train and y_train. Then return the score when we try to predict on our test set. If you get a \n",
    "# convergence warning it's fine, we could modify some hyperparameters here however its generally not neccessary for our simple\n",
    "# task.\n",
    "# RETURN: the model (clf), the accuracy as a decimal number for example 0.633\n",
    "clf, accuracy = trainLogisticRegression(X_train, X_test, y_train, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.9 (4 pts.)\n",
    "# Fill in the scoreTest function to vectorize and evaluate the prediction as well as confidence scores in the prediction given\n",
    "# any text. You may want to go through the sklearn LogisticRegression documents found here:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# to come up with an answer.\n",
    "# RETURN: predicted label (score), predicted confidence interval (confidence) both as decimal numbers\n",
    "text = 'I love the work you are doing'\n",
    "score, confidence = scoreText(clf, vectorizer, text)\n",
    "print(f'Score: {score} with confidence: {confidence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.10 (3 pts.) \n",
    "# Similar to countVectorize, use the TfidfVectorizer to vectorize our data using the TfIdf method.\n",
    "# RETURN: X, y with the same shapes as countVectorize\n",
    "X, y, vectorizer = tfidfVectorize(df)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we re-run the training process to see how TfIdf compares to a simple frequency binning. No modifications necessary.\n",
    "X_train, X_test, y_train, y_test = splitData(X, y)\n",
    "print(f'XTrain length: {len(X_train)}, XTest length: {len(X_test)}, YTrain length: {len(y_train)}, YTest length: {len(y_test)}')\n",
    "\n",
    "clf, accuracy = trainLogisticRegression(X_train, X_test, y_train, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Space \n",
    "Make new cells after here if you need extra space to answer analysis questions or think about problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
